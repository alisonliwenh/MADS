{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 516: Homework 1\n",
    "Version 1.0.20200221.1\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first mrjob script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the following example from the lectures:\n",
    "\n",
    "Note the use of the magic command ```%%file```.  You can use this to write the contents of a cell out to a file, which is what we need to do to use mrjob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q1: Explain what each of the yield statements in the above script do.  Provide a list of what the first few iterations through the mapper() step would yield if the script was run against the ```data/gutenberg/short.t1.txt``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "`yield \"chars\", len(line)` means to caculate the total number of characters in each line (or sentence). The output of this yield statement is to give a tuple with the key of \"chars\" and the number of characters.\n",
    "\n",
    "`yield \"words\", len(line.split())` means to split the sentence by word and caculate the number of words in each line (or sentence). The output of this yield statement is to give a tuple with the key of \"words\" and the number of words.\n",
    "\n",
    "`yield \"lines\", 1` is to yield a tuple with the key of \"lines\" and \"1\". \"1\" means the number of lines in the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the output of running the script against that same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20210704.020331.569433\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20210704.020331.569433/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20210704.020331.569433/output...\n",
      "\"lines\"\t200\n",
      "\"words\"\t1822\n",
      "\"chars\"\t10653\n",
      "Removing temp directory /tmp/word_count.jovyan.20210704.020331.569433...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2.  Repeat the above cell using the the works of William Shakespeare text file (data/gutenberg/t8.shakespeare.txt).  Provide an interpretation of the output (don't overthink this -- just demonstrate that you can find the relevant information in the output).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20210704.020701.106842\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20210704.020701.106842/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20210704.020701.106842/output...\n",
      "\"chars\"\t5333743\n",
      "\"lines\"\t124456\n",
      "\"words\"\t901325\n",
      "Removing temp directory /tmp/word_count.jovyan.20210704.020701.106842...\n"
     ]
    }
   ],
   "source": [
    "# insert your code here\n",
    "!python word_count.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "* `!python word_count.py` means to run the python file named `word_count.py`. \n",
    "* This code creates a temporary directory firstly.\n",
    "* The output below shoes that there're 5333743 characters, 124456 lines, and 901325 words in this text file.\n",
    "* Finally, the temporary directory is deleted once the command get executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q3: Explain what the yield statements in the  above script do.  Provide a list of what the first few iterations through the steps would yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "First of all, we need to identify the definition of stop word. \n",
    "\n",
    "According to *Data Mining*, \"Stop words are words which are filtered out before or after processing of natural language data (text).\" Stop words usually the common words which have high frequency appearing in the articles but deliver low effort of the meaning, such as \"the\", \"a\", \"an\", etc. \n",
    "\n",
    "* `mapper_get_words` statement\n",
    "\n",
    "    - Take a line from the text file.\n",
    "    - If the word from the line is not in the dictionary of `STOPWORDS`, execute the next command.\n",
    "    - Yield a tuple for each word in lowercase from the line and follow by 1.\n",
    "    \n",
    "* `combiner_count_words` statement\n",
    "\n",
    "    - Count the number of time of words appearing in the line.\n",
    "    - The output contains the word and the count number.\n",
    "    \n",
    "* `reducer_count_words` statement\n",
    "\n",
    "    - Sum up the number of words.\n",
    "    - The output contains the word and the sum-up number.\n",
    "    \n",
    "* `reducer_find_max_word` statement\n",
    "\n",
    "    - The output only contains the word having the highest frequencies of appearance in the text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the file against data/gutenberg/short.t1.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20210704.021518.994286\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20210704.021518.994286/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20210704.021518.994286/output...\n",
      "11\t\"day\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20210704.021518.994286...\n",
      "0.8557231426239014\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: Run the above script on the Shakespeare text file.  What answer do you get?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20210704.023030.866045\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20210704.023030.866045/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20210704.023030.866045/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20210704.023030.866045...\n",
      "7.893137454986572\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "`5479\t\"thou\"` means the most appearred word in the Shakespeare text file is \"thou\" and the frequency is 5479 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: What is the impact of removing the combiner from the above code in terms of efficiency?  What does that suggest?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "By removing the combiner from the above code, the amount of running time is decreased dramatically (7.6s to 5.9s). It might because that the `combiner` is repetitive work of `reducer_count`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q6: Write an mrjob script that finds the 10 words that have the most syllables from the t5.churchill.txt file.  Interpret your results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting syllapy\n",
      "  Downloading https://files.pythonhosted.org/packages/11/31/e13c6b0ed7a95f46c3af20df2b995877ea179b88a90ec39122e0621dae08/syllapy-0.7.1-py3-none-any.whl\n",
      "Collecting ujson<2.0,>=1.35 (from syllapy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 11.4MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: ujson\n",
      "\u001b[33m  WARNING: Building wheel for ujson failed: [Errno 13] Permission denied: '/home/jovyan/.cache/pip/wheels/28'\u001b[0m\n",
      "Failed to build ujson\n",
      "Installing collected packages: ujson, syllapy\n",
      "  Running setup.py install for ujson ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed syllapy-0.7.1 ujson-1.35\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_ten_syllables.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_ten_syllables.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import syllapy\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "class MRMostTenSyllables(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "    \n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                syllable_count = syllapy.count(word)\n",
    "                yield None, (syllable_count, word.lower())\n",
    "                \n",
    "    def reducer_find_max_word(self, key, values):\n",
    "        self.list = []\n",
    "        for value in values:\n",
    "            self.list.append(value)\n",
    "            self.new = []\n",
    "        for i in range(10):\n",
    "            self.new.append(max(self.list))\n",
    "            self.list.remove(max(self.list))\n",
    "        for i in range(10):\n",
    "            yield self.new[i]\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostTenSyllables.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_ten_syllables.jovyan.20210704.030238.295794\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_ten_syllables.jovyan.20210704.030238.295794/output\n",
      "Streaming final output from /tmp/most_ten_syllables.jovyan.20210704.030238.295794/output...\n",
      "8\t\"overcapitalization\"\n",
      "8\t\"incommunicability\"\n",
      "7\t\"unenforceability\"\n",
      "7\t\"overcapitalized\"\n",
      "7\t\"materialistically\"\n",
      "7\t\"invulnerability\"\n",
      "7\t\"interrogatively\"\n",
      "7\t\"infinitesimally\"\n",
      "7\t\"indissolubility\"\n",
      "7\t\"indispensability\"\n",
      "Removing temp directory /tmp/most_ten_syllables.jovyan.20210704.030238.295794...\n",
      "8.29448413848877\n"
     ]
    }
   ],
   "source": [
    "!python most_ten_syllables.py data/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The output gives the top 10 high frequencies of words in the text file in tuple for each word and its number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
